nohup: ignoring input
experiment: 0
Processing...
Done!
Processing...
Done!
epoch 10,loss 0.1342,train acc 0.345,test acc 0.289
epoch 20,loss 0.0647,train acc 0.372,test acc 0.327
epoch 30,loss 0.0426,train acc 0.383,test acc 0.315
epoch 40,loss 0.0323,train acc 0.387,test acc 0.455
epoch 50,loss 0.0263,train acc 0.365,test acc 0.390
epoch 60,loss 0.0229,train acc 0.286,test acc 0.364
epoch 70,loss 0.0197,train acc 0.289,test acc 0.313
epoch 80,loss 0.0165,train acc 0.327,test acc 0.378
epoch 90,loss 0.0154,train acc 0.283,test acc 0.252
epoch 100,loss 0.0134,train acc 0.345,test acc 0.324
experiment: 1
Processing...
Done!
Processing...
Done!
epoch 10,loss 0.1364,train acc 0.305,test acc 0.276
epoch 20,loss 0.0672,train acc 0.339,test acc 0.280
epoch 30,loss 0.0454,train acc 0.357,test acc 0.297
epoch 40,loss 0.0338,train acc 0.316,test acc 0.281
epoch 50,loss 0.0262,train acc 0.367,test acc 0.204
epoch 60,loss 0.0226,train acc 0.334,test acc 0.275
epoch 70,loss 0.0195,train acc 0.312,test acc 0.394
epoch 80,loss 0.0169,train acc 0.328,test acc 0.212
epoch 90,loss 0.0146,train acc 0.337,test acc 0.276
epoch 100,loss 0.0127,train acc 0.391,test acc 0.303
experiment: 2
Processing...
Done!
Processing...
Done!
epoch 10,loss 0.1370,train acc 0.300,test acc 0.244
epoch 20,loss 0.0664,train acc 0.357,test acc 0.245
epoch 30,loss 0.0469,train acc 0.289,test acc 0.250
epoch 40,loss 0.0346,train acc 0.277,test acc 0.242
epoch 50,loss 0.0272,train acc 0.318,test acc 0.248
epoch 60,loss 0.0198,train acc 0.467,test acc 0.315
epoch 70,loss 0.0195,train acc 0.322,test acc 0.271
epoch 80,loss 0.0168,train acc 0.342,test acc 0.236
epoch 90,loss 0.0152,train acc 0.294,test acc 0.262
epoch 100,loss 0.0135,train acc 0.321,test acc 0.238
experiment: 3
Processing...
Done!
Processing...
Done!
epoch 10,loss 0.1226,train acc 0.434,test acc 0.245
epoch 20,loss 0.0683,train acc 0.310,test acc 0.246
epoch 30,loss 0.0436,train acc 0.355,test acc 0.287
epoch 40,loss 0.0341,train acc 0.342,test acc 0.224
epoch 50,loss 0.0273,train acc 0.308,test acc 0.269
epoch 60,loss 0.0228,train acc 0.321,test acc 0.299
epoch 70,loss 0.0195,train acc 0.313,test acc 0.278
epoch 80,loss 0.0166,train acc 0.342,test acc 0.266
epoch 90,loss 0.0148,train acc 0.333,test acc 0.295
epoch 100,loss 0.0131,train acc 0.358,test acc 0.219
experiment: 4
Processing...
Done!
Processing...
Done!
epoch 10,loss 0.1288,train acc 0.365,test acc 0.333
epoch 20,loss 0.0623,train acc 0.405,test acc 0.319
epoch 30,loss 0.0447,train acc 0.357,test acc 0.308
epoch 40,loss 0.0304,train acc 0.451,test acc 0.422
epoch 50,loss 0.0254,train acc 0.404,test acc 0.341
epoch 60,loss 0.0234,train acc 0.305,test acc 0.295
epoch 70,loss 0.0196,train acc 0.291,test acc 0.283
epoch 80,loss 0.0177,train acc 0.273,test acc 0.229
epoch 90,loss 0.0154,train acc 0.281,test acc 0.291
epoch 100,loss 0.0137,train acc 0.303,test acc 0.314
experiment: 5
Processing...
Done!
Processing...
Done!
epoch 10,loss 0.1355,train acc 0.322,test acc 0.131
epoch 20,loss 0.0683,train acc 0.299,test acc 0.305
epoch 30,loss 0.0458,train acc 0.281,test acc 0.257
epoch 40,loss 0.0345,train acc 0.277,test acc 0.214
epoch 50,loss 0.0273,train acc 0.304,test acc 0.371
epoch 60,loss 0.0233,train acc 0.271,test acc 0.273
epoch 70,loss 0.0196,train acc 0.285,test acc 0.214
Traceback (most recent call last):
  File "/user/huangshan/course/shenjingwangluo/prj/test5.py", line 147, in <module>
    optimizer.step()
  File "/user/huangshan/miniconda3/envs/GNN/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/user/huangshan/miniconda3/envs/GNN/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/user/huangshan/miniconda3/envs/GNN/lib/python3.10/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/user/huangshan/miniconda3/envs/GNN/lib/python3.10/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/user/huangshan/miniconda3/envs/GNN/lib/python3.10/site-packages/torch/optim/adam.py", line 412, in _single_tensor_adam
    param.addcdiv_(exp_avg, denom, value=-step_size)
KeyboardInterrupt
